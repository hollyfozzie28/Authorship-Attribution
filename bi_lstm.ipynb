{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Tokenization for LSTM\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Seed for reproducibility\n",
    "import random\n",
    "\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets        0\n",
      "AuthorID      0\n",
      "CreatedAt     0\n",
      "text_clean    1\n",
      "text_len      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/cleaned_data_1.csv\", on_bad_lines='skip')\n",
    "num_nan = df.isna().sum()\n",
    "print(num_nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23083404: 0, 267425142: 1, 35936474: 2, 42562446: 3, 460116210: 4, 448562247: 5, 2279776304: 6, 50811932: 7, 416814339: 8}\n"
     ]
    }
   ],
   "source": [
    "possible_labels = df['AuthorID'].unique()\n",
    "#convert labels into numeric values\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "df['label'] = df.AuthorID.replace(label_dict)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "labels = list(label_dict.values())\n",
    "max_len = np.max(df['text_len'])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample training data\n",
    "ros = RandomOverSampler()\n",
    "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1))\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'label'])\n",
    "X_train = train_os['text_clean'].values\n",
    "y_train = train_os['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get vocabulary of words from our data\n",
    "def get_vocab(column):\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    sorted_words = count_words.most_common()\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the sentences by converting them to lists of numbers with padding\n",
    "def Tokenize(column, seq_len, vocab):\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(vocab)}\n",
    "\n",
    "    # Tokenize the columns text \n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(r)\n",
    "    \n",
    "    # Add padding\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocab(df['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of words from training data\n",
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n",
    "\n",
    "# Set dimensions for ther number of features for each transformed word\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Initialize Word2Vec model with the training words\n",
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)\n",
    "word2vec_model.save('../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22606\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocabulary) + 1}\")\n",
    "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (22606, 200)\n"
     ]
    }
   ],
   "source": [
    "# Define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "# Fill embedding matrix with the pre trained values from word2vec corresponding to \n",
    "# word (string), token (number associated to the word)\n",
    "for word, token in vocabulary:\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our split datasets\n",
    "\n",
    "X_train = Tokenize(X_train, max_len, vocabulary)\n",
    "X_test = Tokenize(X_test, max_len, vocabulary)\n",
    "X_valid = Tokenize(X_valid, max_len, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58788, 46)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the datasets into tensor datasets and dataloaders,\n",
    "# enabling data extraction in batches.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test.values))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid.values))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Authorship_Attribution(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
    "        super(BiLSTM_Authorship_Attribution,self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        self.batch_size = x.size(0)\n",
    "        # embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        # lstm layers\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        # Extract only the hidden state from the last LSTM cell\n",
    "        out = out[:,-1,:]\n",
    "        # fully connected layers\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize lstm hidden and cell states\n",
    "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(vocab_size, embedding_dim,\n",
    "                    hidden_dim, num_classes, lstm_layers,\n",
    "                    bidirectional, batch_size, dropout, LR, epochs=4):\n",
    "    \n",
    "    bi_lstm_classifier = BiLSTM_Authorship_Attribution(vocab_size, embedding_dim, \n",
    "                                          hidden_dim, num_classes, \n",
    "                                          lstm_layers, bidirectional, \n",
    "                                          batch_size, dropout)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(bi_lstm_classifier.parameters(), lr=LR, weight_decay = 5e-6)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    return bi_lstm_classifier, optimizer, criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "NUM_CLASSES = 9 # multiclass classification of 9 classes\n",
    "HIDDEN_DIM = 100 # number of neurons of the internal state (internal neural network in the LSTM)\n",
    "LSTM_LAYERS = 1 # number of stacked LSTM layers\n",
    "\n",
    "LR = 3e-4 # learning rate\n",
    "DROPOUT = 0.5 # LSTM Dropout\n",
    "BIDIRECTIONAL = True # boolean mask to choose if to use a bidirectional LSTM or not\n",
    "EPOCHS = 5 # number of training epoch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Authorship_Attribution(\n",
      "  (embedding): Embedding(22606, 200)\n",
      "  (lstm): LSTM(200, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=9, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrosousa/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "\n",
    "model, optimizer, criterion = initialize_model(VOCAB_SIZE, EMBEDDING_DIM,\n",
    "                                              HIDDEN_DIM, NUM_CLASSES,\n",
    "                                              LSTM_LAYERS, BIDIRECTIONAL, \n",
    "                                              BATCH_SIZE, DROPOUT, LR, EPOCHS)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# initialize embedding layer with the embedding matrix\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "# allow the embedding matrix to be fined tuned to better adapt to out dataset and get higher accuracy\n",
    "model.embedding.weight.requires_grad=True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:Validation accuracy increased (0.000000 --> 42.494420).  Saving model ...\n",
      "\tTrain_loss : 1.6878 Val_loss : 1.5758\n",
      "\tTrain_acc : 38.948% Val_acc : 42.494%\n",
      "Epoch 2:Validation accuracy increased (42.494420 --> 51.450893).  Saving model ...\n",
      "\tTrain_loss : 0.9422 Val_loss : 1.4308\n",
      "\tTrain_acc : 68.524% Val_acc : 51.451%\n",
      "Epoch 3:Validation accuracy increased (51.450893 --> 53.794643).  Saving model ...\n",
      "\tTrain_loss : 0.6394 Val_loss : 1.4628\n",
      "\tTrain_acc : 79.181% Val_acc : 53.795%\n",
      "Epoch 4:Validation accuracy increased (53.794643 --> 54.631696).  Saving model ...\n",
      "\tTrain_loss : 0.4874 Val_loss : 1.5244\n",
      "\tTrain_acc : 84.445% Val_acc : 54.632%\n",
      "Epoch 5:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.3976 Val_loss : 1.6612\n",
      "\tTrain_acc : 87.144% Val_acc : 53.292%\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "early_stopping_patience = 4\n",
    "early_stopping_counter = 0\n",
    "\n",
    "valid_acc_max = 0 # set initial best accuracy top 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    # lists to store the train and validation losses of every batch for each epoch\n",
    "    train_loss, valid_loss  = [], []\n",
    "    # lists to store the train and validation accuracy of every batch for each epoch\n",
    "    train_acc, valid_acc  = [], []\n",
    "\n",
    "    # lists to host the train and validation predictions of every batch for each epoch\n",
    "    y_train_list, y_val_list = [], []\n",
    "\n",
    "    # initalize number of total and correctly classified texts during training and validation\n",
    "    correct, correct_val = 0, 0\n",
    "    total, total_val = 0, 0\n",
    "    running_loss, running_loss_val = 0, 0\n",
    "\n",
    "\n",
    "    ####TRAINING LOOP####\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) # load features and targets in device\n",
    "\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "\n",
    "        model.zero_grad() # reset gradients \n",
    "\n",
    "        output, h = model(inputs,h) # get output and hidden states from LSTM network\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_train = torch.argmax(output, dim=1) # get tensor of predicted values on the training set\n",
    "        y_train_list.extend(y_pred_train.squeeze().tolist()) # transform tensor to list and the values to the list\n",
    "        \n",
    "        correct += torch.sum(y_pred_train==labels).item() # count correctly classified texts per batch\n",
    "        total += labels.size(0) # count total texts per batch\n",
    "\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    train_acc.append(100 * correct / total)\n",
    "\n",
    "    ####VALIDATION LOOP####\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            val_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            running_loss_val += val_loss.item()\n",
    "\n",
    "            y_pred_val = torch.argmax(output, dim=1)\n",
    "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
    "\n",
    "            correct_val += torch.sum(y_pred_val==labels).item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "        valid_loss.append(running_loss_val / total_step_val)\n",
    "        valid_acc.append(100 * correct_val / total_val)\n",
    "\n",
    "    # save model if validation accuracy increases\n",
    "    if np.mean(valid_acc) >= valid_acc_max:\n",
    "        torch.save(model.state_dict(), '../models/lstm_model.pt')\n",
    "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
    "        valid_acc_max = np.mean(valid_acc)\n",
    "        early_stopping_counter=0 # reset counter if validation accuracy increases\n",
    "    else:\n",
    "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
    "        early_stopping_counter+=1 # increase counter if validation accuracy does not increase\n",
    "        \n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', e+1)\n",
    "        break\n",
    "    \n",
    "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
    "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "\n",
    "model.load_state_dict(torch.load('../models/lstm_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    test_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "    output, val_h = model(inputs, test_h)\n",
    "    y_pred_test = torch.argmax(output, dim=1)\n",
    "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "    y_test_list.extend(labels.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "Classification Report for Bi-LSTM :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.56      1224\n",
      "           1       0.55      0.62      0.58      1611\n",
      "           2       0.56      0.51      0.54      1811\n",
      "           3       0.47      0.50      0.49      1020\n",
      "           4       0.80      0.66      0.72       280\n",
      "           5       0.56      0.58      0.57      1236\n",
      "           6       0.27      0.24      0.25       180\n",
      "           7       0.59      0.55      0.57      1217\n",
      "           8       0.44      0.41      0.43       381\n",
      "\n",
      "    accuracy                           0.55      8960\n",
      "   macro avg       0.53      0.52      0.52      8960\n",
      "weighted avg       0.55      0.55      0.55      8960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(label_dict.values())\n",
    "labels_string = map(str, labels)\n",
    "labels_string = list(labels_string)\n",
    "\n",
    "# Print classification report for model performance against the test dataset\n",
    "print('Classification Report for Bi-LSTM :\\n', classification_report(y_test_list, y_pred_list, target_names=labels_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
