{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93550186",
   "metadata": {},
   "source": [
    "# BERT LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdb67d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: requests in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pedrosousa/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67c1fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Transformers library for BERT\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Seed for reproducibility\n",
    "import random\n",
    "\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc95f1d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00432cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets        0\n",
      "AuthorID      0\n",
      "CreatedAt     0\n",
      "text_clean    1\n",
      "text_len      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/cleaned_data_1.csv\", on_bad_lines='skip')\n",
    "num_nan = df.isna().sum()\n",
    "print(num_nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e777e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = df['AuthorID'].unique()\n",
    "#convert labels into numeric values\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "df['label'] = df.AuthorID.replace(label_dict)\n",
    "\n",
    "labels = list(label_dict.values())\n",
    "max_len = np.max(df['text_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5336cf",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6333c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6eded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample training data\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))\n",
    "X_train_os = X_train_os.flatten()\n",
    "y_train_os = y_train_os.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d1da53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 6532],\n",
       "       [   1, 6532],\n",
       "       [   2, 6532],\n",
       "       [   3, 6532],\n",
       "       [   4, 6532],\n",
       "       [   5, 6532],\n",
       "       [   6, 6532],\n",
       "       [   7, 6532],\n",
       "       [   8, 6532]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9ffda",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d7c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer from Hugging Face library\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# Tokenize train dataset tweets\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1698d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize tweets using BERT tokenizer\n",
    "\n",
    "def bert_tokenizer(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "            max_length=MAX_LEN,             # Choose max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length \n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04bc938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  77\n"
     ]
    }
   ],
   "source": [
    "# Find the longest tokenized tweet\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)\n",
    "MAX_LEN = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3f39690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all split datasets\n",
    "\n",
    "train_inputs, train_masks = bert_tokenizer(X_train_os)\n",
    "val_inputs, val_masks = bert_tokenizer(X_valid)\n",
    "test_inputs, test_masks = bert_tokenizer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4387e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target columns to pytorch tensors format\n",
    "train_labels = torch.from_numpy(y_train_os)\n",
    "val_labels = torch.from_numpy(y_valid)\n",
    "test_labels = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d99e9e",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acf06d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x7f9c003b2e60>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045133a9",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0055ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 µs, sys: 2 µs, total: 46 µs\n",
      "Wall time: 52.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class Bert_Classifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(Bert_Classifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
    "        n_input = 768\n",
    "        n_hidden = 50\n",
    "        n_output = 9\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Add dense layers to perform the classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_input,  n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "        # Add possibility to freeze the BERT model\n",
    "        # to avoid fine tuning BERT params (usually leads to worse results)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input data to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b53fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize BERT model and define a learning rate scheduler\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = Bert_Classifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # learning rate, set to default value\n",
    "                      eps=1e-8    # decay, set to default value\n",
    "                      )\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Defint the scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    # Define Cross entropy Loss function for the multiclass classification task\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return bert_classifier, optimizer, scheduler, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4decae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/pedrosousa/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS=2\n",
    "bert_classifier, optimizer, scheduler, loss_fn = initialize_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882f647",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca5209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune BERT params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        # Put the model into the evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define empty lists to host accuracy and validation for each batch\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # We do not want to update the params during the evaluation,\n",
    "            # So we specify that we dont want to compute the gradients of the tensors\n",
    "            # by calling the torch.no_grad() method\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions starting from the logits (get index of highest logit)\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the validation accuracy \n",
    "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), '../models/bert_model.pt')\n",
    "        \n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\"*61)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25f444bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "----------\n",
      "Epoch : 1\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   1.996478   |  1584.07 \n",
      "   200    |   1.823699   |  2609.92 \n",
      "   300    |   1.706056   |  1078.92 \n",
      "   400    |   1.578658   |  1128.50 \n",
      "   500    |   1.470867   |  1126.69 \n",
      "   600    |   1.392331   |  1014.36 \n",
      "   700    |   1.316244   |  1199.26 \n",
      "   800    |   1.254623   |  1150.73 \n",
      "   900    |   1.201793   |  1131.56 \n",
      "  1000    |   1.133312   | 37695.47 \n",
      "  1100    |   1.095751   |  1540.27 \n",
      "  1200    |   1.055710   |  1707.12 \n",
      "  1300    |   1.021965   |  3423.23 \n",
      "  1400    |   0.975147   |  1172.87 \n",
      "  1500    |   0.954154   |  1112.55 \n",
      "  1600    |   0.933575   |  1138.55 \n",
      "  1700    |   0.917866   |  1007.48 \n",
      "  1800    |   0.882538   |  964.81  \n",
      "  1837    |   0.833149   |  351.62  \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   1.253482    |  1.405261  |       52.57       | 62529.74 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 2\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.684871   |  1050.63 \n",
      "   200    |   0.612538   |  974.09  \n",
      "   300    |   0.658955   |  966.29  \n",
      "   400    |   0.670497   |  976.67  \n",
      "   500    |   0.624626   |  914.14  \n",
      "   600    |   0.622094   |  929.72  \n",
      "   700    |   0.608451   |  921.73  \n",
      "   800    |   0.604957   |  935.16  \n",
      "   900    |   0.582511   |  942.85  \n",
      "  1000    |   0.560588   |  936.02  \n",
      "  1100    |   0.573425   |  944.72  \n",
      "  1200    |   0.550376   |  942.96  \n",
      "  1300    |   0.539734   |  1651.13 \n",
      "  1400    |   0.543929   |  941.21  \n",
      "  1500    |   0.546558   |  961.84  \n",
      "  1600    |   0.561913   |  960.20  \n",
      "  1700    |   0.527630   |  880.58  \n",
      "  1800    |   0.528145   |  877.28  \n",
      "  1837    |   0.575902   |  358.27  \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.588778    |  1.430815  |       55.61       | 18463.99 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed070ac5",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b06896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate model predictions for the test dataset\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \n",
    "    # Define empty list to host the predictions\n",
    "    preds_list = []\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
    "        with torch.no_grad():\n",
    "            logit = model(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Get index of highest logit\n",
    "        pred = torch.argmax(logit,dim=1).cpu().numpy()\n",
    "        # Append predicted class to list\n",
    "        preds_list.extend(pred)\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preds = bert_predict(bert_classifier, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4579129",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(label_dict.values())\n",
    "labels_string = map(str, labels)\n",
    "labels_string = list(labels_string)\n",
    "\n",
    "# Print classification report for model performance against the test dataset\n",
    "print('Classification Report for BERT :\\n', classification_report(y_test_list, y_pred_list, target_names=labels_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
